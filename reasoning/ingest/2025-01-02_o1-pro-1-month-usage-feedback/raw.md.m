Date: 2025-01-03T06:16:04.563Z
Transcription Duration: 4067.26 seconds
Conversation Duration: 4059.63 seconds
Channels: 1
Number of Speakers: 2

Tom (3.28s): alright buddy you're on
Scott (5.52s): alright okay so i think just before we start the recording you're talking about one of the things you'd like to discuss is o one pro how we get on with it and then add a couple of things to table i've got an update to table which is an update around transcriber i've been making progress pretty good progress actually on a nap called the summarizer and on the commander there's also something i'd like to discuss around the concept of solutions and also for the one covering grinder which is the batch commander
Tom (52.975s): we need a different name
Scott (54.91s): i i i find that name very funny personally
Tom (62.269997s): the there's not many names i'd consider out of bounds but that's definitely one of them
Scott (68.83s): okay so that's my list what's what's on your list
Tom (74.725s): just to reaffirm that the number one priority is delivery of the shortest simplest possible read only crm to the tracking guys mhmm i wanna give an update on that i wanna talk about my experience with o one pro i wanna talk about what i think it means for the broader ecosystem of software development and ai in general and i'd like to table an idea for commercializing
Scott (106.305s): okay since we match on the o one pro thing shall we do that first and then
Tom (112.784996s): alright what is the name of the mode that the commander would be in while this was happening and what are the rules of the communication
Scott (122.2s): i think this is an exchange of information so discussion is not a speakeasy it's more like a sync
Tom (131.33499s): yeah it's a right a report
Scott (135.175s): well it's not even a report because we're gonna go backwards and forwards what we want to do is new
Tom (141.175s): asynchronizing yeah okay shall we say commander synchronizing okay and we'll stay on the topic of synchronizing until the sync is completed so i'll go first because i was already talking i had i have been hitting o one pro from dusk to dawn with payloads of around about the fifty kilo token mark i've been running it hot enough that pretty much all day every day at least some query is running i've sort of gotten good at juggling the plates that that kind of work requires it's starting to feel in a metaphor it's starting to feel like managing managing a team of developers but there's zero time between work specified and work done and so you end up doing you get no break you get all all the managerial effort and the fault finding and the reasoning and the architecting is happening constantly and it's pretty tough actually so in a way i'm thankful that the toil of being a software developer has not disappeared because it means that because that's hard it still makes it a lucrative business to be in during the short time that we have this arbitrage likability where we can make code of very good quality faster than ever before but when everyone catches up there'll still be toil and it will still be hard to do that better like you can still be better than everyone else if you work really hard and you are smart and you got a lot of experience so at least it didn't sort of level that out i had my first hallucination that i spotted from o one pro it was it's in using private data so i can't share it but i was asking i just dumped in about a about a hundred kilotokens worth of xml files and asked it to anonymize the records and so it was dutifully writing out the anonymized versions and one of the one of the xml tags was called support payments and it made it have like support or it it was some kind of funny word that a developer had chosen and it caused there to be like three p's in a row in the tag so it was like sup payment and and the hallucination was for four times out of like it must have generated maybe fifty kilo tokens output but four times it got the tag wrong and it used four p's instead of three p's in there and it did it four times without and that's the very first it was a hallucination that i instantly spotted obviously because the xml didn't pause but that's the first time i've seen it and i've been slamming it hard so and that's a pretty i mean that's significantly better than a human error rate if i'd asked the human to do the same task so all in all looking pretty **** good but there's a new type of toil over to you
Scott (392.095s): remarkably similar so i've been hammering it with larger token payloads from comcast mhmm and it's hides output that it thinks you don't need to know even though you've said actually give me this information if it thinks actually you really don't need to know a bit like a a really good pa you know which is slightly irritating but from the reasoning you can see that it's considered the whole thing a transcript given over and saying give me one minute by one minute a summary report was talked will end up seventeen minutes even though it's like fifty two minutes and even though i tried to pistol with it it goes no no no you don't really need to know this which kicked off the the whole thing we texted about yesterday around how do you assess input from us and how useful it is and it seems to be throwing out a lot in terms of output so it tries to make the output more succinct and sometimes more succinct than we want because we want to stash that output for future use but it still considers it inside of the thread second one is it's much harder to test it with
Tom (494.135s): yeah because we don't have sys sysprompt instructions but those are there for us if we use o one full but o one pro is not in the api yet yeah and i do believe that o one pro is superior to o one even with high reasoning
Scott (514.05s): yeah i'm looking forward to that because things like only when i ask you for an output i'm needing a markdown formatted output and you should keep on repeating i guess sometimes it just like goes i don't think you really do want that it's really irritating well it's
Tom (534.86s): good that it's got opinions like you know like it's it's i think our job is to really fall in line with it so that it works with us rather than against us i've noticed an oddity in the the way that it sort of postures itself when it does updates like if it writes me a read me file and i go no no i wanna drop the constraint that only one process can write to the moneyworks branch blah blah blah it's like okay and it goes and updates the readme but it mentions the fact that we are dropping the constraint like it doesn't write the readme as though it's like fixed for all time it writes the readme with some hint at what the readme used to be that's annoying i'm like yes i'm getting that a lot
Scott (586.34503s): a lot with the stocks when i'm asking it to update stocks it will tell me about the delta and even then it will give me a summary of the changes when actually it's like just do it
Tom (602.77496s): right well perhaps we're using the tooling wrong there because the winning theme seems to be if you can get o one to do or o one pro to do the top level reasoning and then cursor then it can go through and roll it all out would you like to see a cool demo of that in action
Scott (630.95496s): yes and also to reinforce what you just said i'm doing the same thing by switching models so previously we're talking about different sys prompts
Tom (639.675s): right
Scott (639.995s): on the same model i'm switching models for almost like a hierarchy in a inside a company it's like like just do it or you know do something interesting and
Tom (652.25s): right so you call on the dumb model when you just need things done blindly like there is a thing that's too smart right like it can be too smart yeah okay so here here's an example of what i'm gonna do here is i'm gonna i've just had quite an argument with o one about the fact that how to synchronize data between the the rest of the application so the web app making changes to the data and the moneyworks accounting system itself and i've been arguing with it being like you know it keeps trying to wanna use git moves to synchronize what the changes were with what the like i wanna have two branches i wanna have a branch where one branch is purely the view of the moneyworks server data and nothing else and the other branch is what the application wants to change in the moneyworks branch and then the process will look at those diffs make the change on the server which causes the main branch to update and that's the only way you know that your changes are made it never actually goes and merges those two git branches and it made me think am i weird for wanting that like that seems clean to me but the bot's just like every every file that does at the end it's like and then you merge the changes branch into it and i'm like no
Scott (756.72s): i've i've experienced something similar to this one my question is on this thread have you ever mentioned git
Tom (765.12s): oh yeah all over the place
Scott (767.36s): ah right so i've seen this in that it's uses if you mention repeatedly a thing and then ask for a completely clean slate it doesn't do that you can't get a clean slate
Tom (786.67s): you can't get a clean slate
Scott (788.51s): no because in this case git is mentioned all the way through mhmm it assumes that you are a git guy
Tom (796.59s): oh like you're saying that even it sort of makes it sticky like if it hears the word a lot and it felt like it wanted to do something a certain way even when you tell it not to yeah so i guess my gut feeling is that the quality of the output is always best on its first reply and so i always when i'm not getting my way i just refine what my first prompt was and then run it again because i often don't care about the back and forth i really just want the first thing run again but the issue is that that's slightly slower because it doesn't use cached tokens and that's you know that's okay when we're on flat rate but when we start to pay for our own tokens that's gonna hurt plus the performance hit is a bit bad so it'll just be it'll just be way nicer once we get access to omen pro and the api it'll be nicer because we can load all the context up and be like bang and that's our cached tokens and then we just can like restart what we say in there over and over until we get a response we want plus use the system prompts
Scott (872.5s): i agree and one tactic i've been using that gets reasonable results is if say you've got it's assuming that you're a git guy you can prompt and say forget everything about git i'm not talking about git because i once i actually accidentally deleted its memory of a thread by saying forget everything i said and it promptly forgot it which is quite interesting and so i've been getting it to forget ideas forget
Tom (917.005s): anything
Scott (917.405s): to do with you know x y z
Tom (919.885s): forget ideas yeah
Scott (923.965s): and it seems to work and i don't know if you want to try that as a experiment here to see if it
Tom (929.805s): works with you but
Scott (931.66003s): it's not a hundred percent but most of the time
Tom (934.86005s): i can't imagine that working for what i'm doing just simply because it it it's so it's so close to what i want that it's hard to say what i want and also forget like the the issue i'm having is because it's so close to and and probably the overwhelming weight of the internet would probably say yeah you should do that merge that's what i'm having trouble holding it off of and it maybe it's my design is bad maybe that's a sign that my design is incomplete or perhaps i've tried to approach the problem with too many variables and i should have just focused on okay first we're gonna get the moneyworks branch synchronized and then we're gonna call that version one of the module i may have been trying to do too many features concurrently without iterating up
Scott (990.08997s): i found the similar similar problem in that the commander that i the nap commander that i just committed you know is is much smaller and the reason i was making it much smaller is because it gets a better result if you ask it fewer things upfront
Tom (1015.945s): right right right right
Scott (1017.76996s): if you ask a whole bunch of things it tries to be very helpful and summarize and tell you what the diffs are and so on but if you give it something smaller it takes its time and thinks and does all that good pro stuff gives you a high quality answer and then the second prompt you can add in what you're actually really should have had it in the system prompt as well inside of the process and this is where commander is standing at the moment right
Tom (1053.38s): okay
Scott (1058.115s): the you had another point there though can you run through what are your points again
Tom (1068.195s): it might be a sign that you're wrong if the bot keeps steering towards some particular place it might be better to build things up incrementally like i've tried to i've tried to implement multiple features all at once here with this code module and maybe that's a mistake i should've got a a simple one working and then evolved from there is any of that hitting hitting the spot
Scott (1097.1951s): yes the incremental one is a shared experience where right breaking down what should be a single shot i think it's trying to be too smart and remove information
Tom (1110.3301s): yeah yeah yeah yeah i think i think it's our folly because because this thing is so smart we're tempted to load it with concurrent tasks but actually it's really smart in a weird way like it's very good at a narrow thing and it will it will talk about all these surrounding things while it's doing that one thing very well but as soon as you try to get it to hit multiple targets it's like it starts to get kind of shaky in a way that i've never experienced a human having that kind of mental degradation because of multiple targets it degrades so gracefully that you almost don't notice it but then you're like it's it doesn't degrade like four o where it hallucinates and makes mistakes it makes kind of well reasoned errors and you're like that's still reasonable but you did not listen exactly sort of thing
Scott (1166.0901s): yes and that's that's my experience as well it is it's gone beyond beyond the only time i've seen it hallucinate is where i've accidentally missed out definitions from concat and it's had to derive what a nap means and you know it's got neuroanction
Tom (1186.75s): yeah it made it up yeah so i've still never seen a single hallucination or error i've never caught it out once and i think that's remarkable like before you catch out on on the rig like every fifteen minutes you're like oy but not this one
Scott (1206.525s): exactly so i think we've moved or pro has moved beyond hallucinations but there is a danger here because it sounds so reasonable and so close but if you haven't given it the right information or it's derived that information from its training or from your context and thread it can it can actually go off piste but quite reasonably off piste
Tom (1236.245s): yeah reasonably off piste so i i i had a go at it this earlier today about something and then i was like no hang on i'll get it to tell me why it thought that and then it listed out quoted the files that i gave it that said why it should use git merge and i was like oh that's that is what the file says that's a pretty reasonable thing and so i had to go back and fix it up that's right
Scott (1261.895s): right that's right and on your your renaming example i've had a couple other renaming examples but it's mostly because if there is a a gap in this knowledge i think it goes out to its training and then tries to fill it with something reasonable mhmm okay which with the burden then is on the human operator
Tom (1286.985s): yeah you still gotta read it abe you still gotta spot it that's sort of the skill yeah okay so i got a demo here which indicates what i think is gonna be our future just like how there's jokes about turing hitting the first bug spending days on it and being like oh dear i guess that's gonna be our lives now our lives have changed a little bit and it seems like the pattern is high level reasoning at an architectural level in a narrow scope like in the project map file which you see here
Scott (1323.3551s): so where is this in nap's crm
Tom (1328.61s): moneyworks sync moneyworks sync sorry ah there moneyworks moneyworks money works
Scott (1343.255s): do work most of the time yes right okay go ahead
Tom (1347.135s): and so the the repeated cycle appears to be a a narrowing and a widening of the intellectual aperture of the ai and so what you do i think is you come in you you get it to reason through these high level views right so you like state the problem very clearly in the readme so the readme has like you know all the wrong files sorry the read me of the money works sync has like a very well laid out description of everything about this package i included some additional information which was the process map which describes basically the algorithm and how the algorithm should work and so these are like high level high level well reasoned well debated well defined documents that express what should happen so this is the narrowing of the aperture and so i'm talk are you holding my hand up to ask a question
Scott (1413.08s): yes ma'am yes question is this edited at any point or is this entirely generated
Tom (1420.92s): it's the process entirely generated i reviewed it and i went through to get this i went through maybe five loops five prompts to get this
Scott (1432.265s): right so by editing really you're going back to the reasoning as opposed to going in and saying oh that's just wrong
Tom (1439.145s): it's it's a waste of time going in there and typing by myself because one of the minor frustrations i have is that just simply because i have to cut and paste between o one pro it's it's easy to get your edits overwritten if you read it them by hand now i'm sure that when we have tooling in place it'll be better but what i'm trying to sort of hint at here is that there's a workflow that i can see forming where edits doesn't really matter and edits in the keyboard become really just a way to do very precise little snips but the key the key cycle appears to be you reason about these top level documents get them right then you start drilling down the application of it where you focus sort of one file at a time one folder at a time so for example i would come down and go okay i wanna do i wanna get into the the tests folder and i wanna write all these tests and so you go in and implement it and while you're implementing you may need to change the higher up reasoning sometimes but ultimately the idea is you get down to the bottom and then you go okay done scope back up to the top level you've got the working thing now you deal with things like issues defects extra features reason at the top and then mush it down again and so the it's it's this sort of inhale exhale inhale it's this repeated like open high level low level crush high level reason low level crush and you keep looping that around around and that's where you get your your gains from but it starts to look like this will be my last statement then i'll pass over the conch it it looks like like a merkle tree almost where you drop something at the top and then you see this digga digga digga digga changing as you sort of roll down the tree
Scott (1597.3949s): take transcripts there seems to be two sources of files one that the ai never touches and humans only ever touch and the output as a rule i've not been touching any output if the output is wrong then i go back to the prompt and then i
Tom (1623.51s): why do you have that rule
Scott (1627.135s): because sometimes for example if i'm saying okay so create me a new priority list and i'll change this the prompt sorry for the stocks it will generate a new stock and they'll look at the diffs and go no but rather than go in and change the stocks because i know it's gonna be wiped out next time i go back up and then try and reason with the ai to get the stuck in to not do the thing that i'm spotting is doing wrong
Tom (1660.3651s): are there any cases where it would have been faster just to edit the stuck
Scott (1664.97s): yes but next round next transcript the stocks are just gonna be overwritten again
Tom (1672.65s): so can't you feed the stocks in and require that they be modified instead of flattened
Scott (1680.825s): yes however this is one of the topics i was wanting to talk about which is actually interesting and called topics because there's this is another agenda item i don't want to go into it at the moment okay but we'll pick up on it next time but there is a long longitudinal slice that you can give to the ai that allows it to perform much better
Tom (1704.04s): because i guess the the the part of the work that we're doing with o one is familiarizing our sales ourselves with it and to understand where there's a bit of an art to doing the edit directly versus fixing the prompt is those kinds of getting a feel for when that's when that's right versus not which is the same as managing a person you know sometimes some tasks easy to just do yourself definitely the top level architecture diagrams you should do those some little changes you should do those things like that right so i'll show you this this demonstration of how that sort of how i've been working there so i got this top level project map in from o one after a little bit of wrangling and then i've just asked claude three point five over here to modify the file system so the file system is the moneyworks structure here and i'm just simply asking it to make all the changes to the file system that this file specifies and i'm doing it in the what's called the agent for for cursor it can run commands and stuff as well like it can create directories and it can do other stuff like that but what i wanted to show you was just get a glimpse of where i think our system needs to get to where you can watch this thing just whirring away grinding away as you may say and just you can see it creating these files as it goes building up the functionality building up the following on just on the information that was given in this in this file
Scott (1823.275s): picked up the wrong handset there question is gemini editing and updating these files directly without you having to look at a diff
Tom (1842.11s): claude is so it's it's claude three point five sonnet and it's running directly creating files and updating files i don't understand the question
Scott (1861.8099s): you've just answered it okay the reason is what you've given is a file of instructions to order a bunch of other files
Tom (1870.7699s): yeah that's the thing that's the thing
Scott (1872.61s): just does it
Tom (1873.405s): yeah that's the thing that's beautiful that's the thing yeah
Scott (1876.365s): yeah
Tom (1876.605s): yeah now obviously it's gonna not get the code that it puts in like let's just check out the money works sync so that's not that's not right but the point is that i'm able to use this to stub out the files and then i'll pass those using concat back into o one and be like i wanna focus on this one specific file make that file so yeah that's that's that demonstration
Scott (1917.43s): that's that's a neat little heuristic or tactic okay good i think on your previous point which was there is a new skill set i think you're absolutely right this is not simple stuff
Tom (1942.51s): nah it's hard man it's really hard
Scott (1946.51s): in some cases
Tom (1947.5499s): it's harder than ever before
Scott (1950.11s): if you have the discipline of only doing that background the ai loop and you have these files what i've been doing and this is working well these files i will not edit but i won't allow the ai to edit until i've seen them that works well and these files i do not allow the ai to edit that seems to get some clarity to the whole process
Tom (1981.6499s): yeah so i guess what we'll do is we'll we'll we'll go around we'll we'll we'll carry on what we're doing because clearly it's faster than anything that's ever come before like we are amplified now oh yeah but we need to get we'll get the i think it's a good path to get the crm installed because we learn so much about these processes and then we'll capture this into a platform if we don't do the hard yards now we won't understand enough of what these tools need to be so i it it's this is really hard this is probably the hardest work i've ever done in my life it's like a month's worth of project management done in a day and you're like ouch my brain my poor little brain
Scott (2038.0299s): my experience is similar it it does feel a lot like having a bunch of really keen and smart people just waiting to do exactly what you say mhmm and i've started getting a bit more disciplined on my tabs just because you kick off something it would take two or three minutes you go to something else kick off something else kick off something else and then you want to go back and so on and so there is a kinda human level discipline that's needed in order to actually make good progress and really grind the thing which is interesting the second thing is understanding what each tab what were your intention was and so i've got this on the whiteboard at the moment this tab here's what i'm intending to do because you can go back to it names the the thread inside of pro and you can go back and say oh right that was a good idea i need to go back there the cut and paste thing is actually just dwindling i'm spending most of my time in pro and cutting pasting is almost like a commit it's like right okay alright i'm happy with that let's go over and the commit is real crazy two thousand twenty five zero one zero two hey our first note of the year declaring goals of threads if we state that we are trying to do then the bot can help us work better yes yes signposting is really really important to pro now question is if we get i'm not i'm
Tom (2149.41s): not gonna get a chance to talk or you're still going
Scott (2153.0352s): you're still going you're you're conch if you understand what i was about to say or anticipate what i was about to say
Tom (2159.115s): well i was trying to say that i have written this little file here that represents exactly what you're talking about the new skill seems to be that no matter just like throughout history no matter when obviously after the first computer was made but since then the fastest computer on earth has always been as big as a building or bigger so it is with ai the greatest ais are always going to be a little bit slow no matter what no matter what the frontier ai is slow therefore your skill becomes can i manage multiple plates spinning in a room effectively one of those tactics we're picking up many of them now one of them is this it would be good to actually state what the goal is or what you're trying to do so that you can sort of come back to it you got a quick summary you can jump back in real fast ai can do some things for you but a way that i've been dealing with that in the short term is naming the thread so there's a little bit of a bug with o one pro that if it's busy grinding and you name the thread it can sort of require you to regenerate but as a general idea because it names the thread kind of randomly i just been renaming it here to keep track of what it is we can also try maybe the opening line being like i want this conversation to be called blah blah blah and you name it in the top of the thread maybe maybe that would help
Scott (2251.075s): i that's a good idea the maybe it's an idea to because we want to also share threads i have some kind of naming convention for threads no
Tom (2261.75s): i don't want a naming convention
Scott (2265.03s): okay alright that's fine the no over to you i'm done
Tom (2275.995s): with it
Scott (2276.875s): i was about to get going okay great
Tom (2280.875s): i got a thing to show you i'll just send this off to run while we're while we're going a technique that i developed is i've been generating high quality docs so here we have moneyworks is actually where are we hold on a second sorry something is wrong okay so here's an example of the documentation from the website of the vendors of moneyworks that i've got a one to process and turn into helpful little helpful little markdown files which get included in the concat and that's kinda that's sort of the the key that i've been picking up on is that if i can pull in the right stuff then you know like here's the the database schema here's more of the simplified database schema here's like the the schema of the all the stuff right and the way i've been doing it is i've been going into i'll show you how i do it are you interested in how i do it
Scott (2392.165s): yes please
Tom (2392.885s): yeah okay so i have found that the best way is to go into whatever it is that you're trying to convert and to pick whatever it is you want then you go right click inspect and then you pick the thing that you want then the html the dom node that you want so like for instance i want that one i then copy the whole element
Scott (2432.57s): mhmm
Tom (2433.6902s): and then i go make this into a markdown doc and it does a brilliant **** job
Scott (2447.1948s): nice work sir yep excellent i had not thought of that that is good
Tom (2455s): and so here you can see that never mind give me a second i need to catch my thoughts floor is yours this is basically what's okay this is basically what's been shattering my brain is this like jumping around between all these different things all the time and like how am i my head it just i'm just constantly across multiple different things oh yeah that's this this process of doing this generation caused me to stumble upon another item i'd like to table which is in the realm of context generation we can table that at some later date i have to go in exactly twenty five minutes and i cannot be late this time because i'm dying i need to sleep
Scott (2515.7102s): okay we've got twenty five minutes excellent okay how do we best use those i i get what you're saying here
Tom (2526.9248s): mhmm
Scott (2527.7249s): unless there's anything new you want to bring into it i'd like to bring you up to date with the things that i am tabling
Tom (2536.8s): okeydokey first thing just to check am i doing the right thing by making the i think i'm doing the fastest possible version of the the crm is that the right thing to be working on
Scott (2554.935s): that remains your number one priority according to the commander yes
Tom (2558.375s): yeah but there's multiple different types of priority like getting a tool out for you like a fix to the transcriber
Scott (2569.0698s): okay so speakeasy i've got model paths to get the transcriber out or to get transcription out the transcriber seems to work fifty percent of the time i still don't know why only fifty percent of the time and sometimes it's even if i open a new tab it works so it's not our problem it's youtube's yeah
Tom (2590.125s): problem yeah so i learned that when you're that library looking at the issues that got discussed youtube blocks you by different blocks different things in different regions and that's why you and i were having different experiences because you're coming in from the uk i'm coming in from singapore and it's like oh that's you know it behaves differently so i made the changes that they recommend in the issues but i haven't pushed it up i reckon i can get that out if i spent half an hour on it would you like me to do that i think you might find it more reliable or if it doesn't matter
Scott (2626.51s): but that that would save me time because if the trust camera doesn't work then it involves moving to
Tom (2632.19s): a different tool right so maybe thirty minutes elapsed okeydokey alright i'll get that out for you then while we're in speakeasy this is repeating but showing a higher quality output of transcribing the openai docs and so i did that same a similar trick on that but see the issue that i where i have to spend individual time is when they when they put an image in that won't do like the concat can't deal with an image so i always have to go back into o one and get it to turn the image into some kind of an ascii diagram often that can take four maybe five loops to get a good representation because it's a little bit off sometimes but generally speaking that's what i've managed to do and then i end up with these docs and so it ends up with a set of docs that are bot friendly right which is a kind of a new a new thing to be having i suppose so yeah good
Scott (2707.255s): good
Tom (2707.895s): yeah right and so he is like like converting this diagram took me like seven goes because it it just couldn't get it but then i had to come in and say actually make that a sequence diagram and then the sequence diagram i got it first popped so there's a bit of an art to knowing what diagram you want
Scott (2729.545s): so yeah okay
Tom (2732.025s): okay so we're gonna what do you wanna do end speakeasy and begins begin table
Scott (2736.265s): yeah end speakeasy and we've got twenty one minutes by my clock
Tom (2741.76s): okay and you're at table mode you have the floor
Scott (2745.84s): right i've got one two three four five some things to go through perfect right actually no six we've already talked about transcriber so that's that is done and now i've pushed a new version of the summarizer which is in what i'm thinking about is very similar to what you've just talked about in that if you give summaries for the bot to think about as well as the the raw to actually grind then it gets much better results and so the summarizer that i've just pushed identifies topics within the the ingest whatever it is transcript is the one i'm doing based on those latest transcripts sorry i'm reading from my screen here which is why i'm leading up i'm marking out the changes based on the context so
Tom (2821.2651s): if
Scott (2821.425s): you can pick up pull up github and look at a summarizer map you'll see where i'm going it's much smaller than my previous summarizers but it seems to be it's almost like a a pre step to informing the commander about what's just happened as opposed to just getting the commander to do the
Tom (2849.365s): so so what are you you're what are you tabling
Scott (2852.0051s): here no i'm reporting not tabling reporting
Tom (2857.125s): you said we're in tabling
Scott (2859.42s): okay my apologies this is just reporting and
Tom (2864.22s): does the does the reporting cover the six things that you have to go over it's six items to report on
Scott (2872.22s): i the things that i would like to table is the use of the attribution model which i texted over talking about how useful these conversations are and what the i think is a useful metric for attribution and the second one is the use of longitudinal topics for the grind the grinder oh i'll think of that now so those are the two things
Tom (2905.805s): so i'm confused now what mode are we in right now
Scott (2909.9648s): i'd like to report to bring you up to date or to point out some things and then i'd like to table those two things
Tom (2918.49s): okay okay so carrying on in reporting mode i'm in the these are the two folders that you changed
Scott (2926.97s): i if you want to
Tom (2928.09s): one is the transcripts and the other one is in the naps folder within reasoning yep summarize
Scott (2936.525s): summarize transcripts yep now first question is given what we're previously talking about saying we'll just use raw and summary i've changed this name to nap
Tom (2948.49s): yeah so in that case is there ever gonna be is there ever gonna be a second or more files in here maybe it could it could be
Scott (2961.0352s): there could be additional oh pointers and so on but i don't foresee at the moment but yeah
Tom (2968.395s): is is is this an example of a prompt
Scott (2972.235s): at the moment all of the naps i've got in there are basically prompts except for i've stuck in well effectively what are bash scripts
Tom (2983.29s): and you're saying to me that what you want is you wanna take this prompt and output a report in this format and that's what that's what the summarizer does
Scott (3001.38s): yes yep yep and while it's inside of naps slash reasoning then this is just for me to mess around with and we've also got that move to one layer up which is
Tom (3016.865s): the actual naps what's the context what's an example of context that you would give is that like here's the domain
Scott (3025.7449s): it's the output of the concat of the ah
Tom (3028.705s): i see okay yeah i see consider okay awesome okay cool so that's the first one i can't actually see how well this performs without running it myself and going forwards we would need to have some evals and some expectations around how to score it and i think this is stalled while we wait on being able to have sys prompts that run-in o one pro and so i take it that my action from this is simply to acknowledge and comment but no further code action is required
Scott (3066.95s): correct okay likewise with the commander which is there yep so i've reduced that but again it comes down to context and here's the the context is the concat and in this case the transcript is your input
Tom (3096.12s): what's the difference between concat and input the input is not drawn from the existing state it's new information so concat is the existing state the transcript is the input which is the fresh information so that's okay but the way i would prefer to receive that is that the new information was a file that was part of the context as well because if you pass it in as input it means it came in through the the speech channel and it's it makes more it's more elegant in my opinion to pass it in as a file and then talk about it rather than stuff it in as the input does that sound the does that sound like you still get a good quality output from that
Scott (3161.255s): can i clarify so the output from concat yep and the new information as one single file
Tom (3169.3s): one concat because because it's all context and how i'm thinking about doing context managing context in the surface application that we have it would be first of all the contents that you are trying to use as input would have come from the file system anyway and so it's sort of like i mean are you asking me for a distinction where you're like some files are considered background and this file is special in some way or do you think it would be sufficient if you just mentioned that file by name and supplied all the files
Scott (3210.8s): i run some experiments and find out
Tom (3212.905s): what the results are but okay
Scott (3215.0652s): effectively this new information given the context and your instructions
Tom (3220.745s): right
Scott (3221.0652s): what what you're gonna do right
Tom (3223.385s): okay that's how i would prefer to receive the the commands it's it feels nicer for me but furthermore i think the general direction of ai is such that the evals that you get from doing it one way or the other will eventually settle down or saturate at it's the same either way and if it was gonna be the same either way then it's much easier to present it as just all context than you just mention that file in your chat does that sound reasonable
Scott (3259.48s): it does the aim that i'm trying to to get here is for it to look at the new in the context of the old yep but i'll run run some experiments run some experiments
Tom (3276.805s): okay awesome
Scott (3278.325s): okay so next
Tom (3279.365s): one and
Scott (3280.645s): we've got twelve minutes left right so we talked about commander it is going along the same model of context and input the actual output can be modified and so on and that's all fine the solutions i'd just like again to report i have but i have not yet committed a bunch of solutions from our previous transcription
Tom (3313.855s): we had agreed that they were called remedies why have they been renamed
Scott (3319.38s): i because of my ****** memory probably okay i will change it to remedies okay it's an empty folder at the moment and just a live thread so no biggie okay okay now i'd like to table the idea
Tom (3343.555s): of how how how shouldn't i get i should get some questions right
Scott (3347.1s): oh yeah go
Tom (3353.9802s): ahead let me just think is this still providing the context as the basically the whole folder and you're just saying hey off you go to the commander as opposed to focusing the commander on a specific task and going coming at it from multiple different angles like give me all the new ideas give me all the contradictions give me all the task updates give me all the whatever currently the i give in the thread because
Scott (3408.6501s): we don't have a system prompt the what would be a system prompt and will become a system prompt to the commander and i'm saying give me you know new ideas give me any updates to stocks that already exist
Tom (3421.245s): but you do them in separate stock separate clean threads is that correct
Scott (3426.47s): no i do it as a single system prompt at the moment
Tom (3430.39s): and you ask for all those things all at once
Scott (3433.5898s): well this to go back to what i was saying about giving it overloading it i've rolled it back to not doing the entire commander's job in one batch
Tom (3446.395s): i think i think that we had i was trying to drive at that point i still strongly believe in that architecture that your performance will be superior and i think this will hold no matter how good the models get your performance will be superior the more focused you can ask it to be and i think that the focus needs to be correlated to a single file or folder that you're asking to be updated as a result so you you match one for one the the task you give the commander the update zone that you wanna hit the entire context of everything but then you're like this zone i want you to update like all the stacks or all the definitions or just this part of the dream catcher ai definitions when you treat the commander like that that's when i think you'll get the best results that's what
Scott (3504.1848s): i'm looking for okay and i think that is that would be ideal however at the moment so there are a number of outputs from the commander identify what needs to be changed in the existing stocks identify any new stocks identify any new solutions and identify new priorities given the context of the transcript that you've just been given mhmm now identifying new stocks and updating old stocks both need to be considered when identifying priority so can i just clarify what you're saying is it will be better breaking commander down into each step such
Tom (3554.1348s): as yep
Scott (3555.095s): give them this identify what stocks need to be changed and then take the output of that yep and effectively merge it yep copy and paste it into another one yep in order to get
Tom (3564.775s): the final yep okay
Scott (3566.51s): i i think so and eventually obviously that's gonna be easier to do yep okay so good alright so any other questions
Tom (3579.745s): no further questions sir
Scott (3582.145s): on the next tabled idea or on this tabled idea
Tom (3587.825s): commander we table ideas
Scott (3593.44s): i would like to consider and spend some time thinking about longitudinal topics because at the moment we're giving the commander
Tom (3605.52s): what's a longitudinal topic
Scott (3608.135s): a topic that has progressed over time as opposed to just the last transcript or the last ten transcripts
Tom (3615.9749s): so a topic is a good example of a high value longitudinal topic one that had been hinted at or skirted around or gestured at across multiple conversations but the humans lacked the precision or the vision to be able to declare it exclusively or as a single topic is that the type of thing you're looking for
Scott (3638.3252s): correct and i would add in that now it can be considered with the vertical transcript commander saying you guys were talking about this thing last month and you've been talking about it for the last five years and here's the consequences of what you're doing
Tom (3664.5151s): it would seem beneficial to not just extract a longitudinal topic but to point at it as a as a point of order that needs to be declared because we have breakthroughs when we see the thing that we've gestured at for so long and we're like oh that's just how it is and then we move on so that would be that would be a great candidate for a background thread or a batch processing job that just like grinds over like everything right
Scott (3698.865s): exactly i was trying to not use the word grinder but that's what i've been calling it to this point but that's effectively where it is it's a long longitude
Tom (3705.58s): oh that's what that's the piece you wanted to call grinder yeah yeah at the selfishly i'd already used the word grinder for another part of the system and it's it's also not a great term to be publicizing a lot but the act of grinding is a recurring strategy that applies not only to longitudinal topic extraction but to a great many other things and that's why i'd argue that it's not a great name for the process but of of longitudinal topic extraction but it's more a strategy that you'd go about it
Scott (3746.05s): i'll come up with a new name my question is is this worth me spending some time doing now okay
Tom (3757.025s): this is this is a scheduling thing this is not part of tabling has the idea been totally tabled and you feel like the concept has been stated clear enough that it can be recorded
Scott (3767s): i'm tabling it in order to see whether i spend more than the
Tom (3773.24s): two hours that was not my question that was not my question second has the idea been tabled sufficiently
Scott (3783.4749s): do you think
Tom (3783.795s): i think i think you have an issue you have a different a problem with distinguishing between the tabling now this is not speakeasy this is in the middle of tabling because i'm trying to see if tabling has finished or not and i think that you actually have a problem in separating out the presentation of ideas with the discussion of the implementation or the execution of the idea
Scott (3807.7551s): maybe there is a need for another commander mode to move between those my question to you is do you have any questions about how what i'm proposing is it no
Tom (3824.79s): i think i think it's so long as you feel like i understood your idea and you didn't have any more insights to add as to how we would implement that because the techniques to do that seem like a very lengthy study with many options which i don't have immediate answers for you so long as you weren't diving into that then i i feel like i get the idea completely
Scott (3850.86s): okay good
Tom (3852.46s): okay so tabling is complete tabling is complete and now your scheduling question well no because prioritizing question
Scott (3864.315s): no the scheduling will be down to prioritization by the commander not us
Tom (3869.515s): am i allowed on it is it useful to have an opinion on it you can alright hang on what am i trying to add i'm trying to add a i'm trying to add some information about what the necessary path to implementation is that can affect the priority due to increasing the cost so at the moment the the the process you're describing is a great process but it is very human labor intensive to do without automation tooling and that's if you knew exactly how to do it but you don't know exactly how to do it and so you have to experiment and so you've got a very laborious process with many possible options which equals labor squared which is a bad use of labor right now in my opinion because if we labor on getting the automation platform up then we can tackle this highly fruitful and lucrative area of longitudinal or background microwave noise topic extraction
Scott (3941.9949s): okay i agree and i'd like to mark that as a solution
Tom (3950.2349s): as a remedy
Scott (3952.2349s): sorry remedy no okay good so moving on because we've only got one minute left
Tom (3959.29s): right it's like a it's like a remedy direction or something like that
Scott (3966.33s): remedy yeah well remedy doesn't need a nap or yeah
Tom (3971.655s): it doesn't have to be a full thing
Scott (3973.5752s): yeah yeah yeah okay and the last one is we don't have time for because we only got one minute but any thoughts on the attribution based on the information new information and old or regurgitated information or placers
Tom (3992.75s): for attribution yeah that's not attribution as such that's metrics and i think that's very valuable to have as a dashboard because you score it i think your attribution is gonna be independent of how how concentrated your contribution was what you're talking about is contribution measurement which is not the same as attribution like you're talking about mechanisms by which we can increase the rate of contribution which is an absolutely great thing to be into and i'm all for it and we should make dashboards that can give us these feedbacks live while we're talking but that is not attribution
Scott (4043.5298s): i i agree and thank you i propose that we make that a remedy and we'll come back to
Tom (4051.5298s): it okay
Scott (4055.45s): that is time you need to go
Tom (4057.45s): you cannot be late thanks buddy i'll i'll do the upload